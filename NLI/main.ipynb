{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_hw4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnhcqv11z56M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/My Drive/hw4/pnli_train.csv', header = None)\n",
        "val_df   = pd.read_csv('/content/drive/My Drive/hw4/pnli_dev.csv', header = None)\n",
        "test_df  = pd.read_csv('/content/drive/My Drive/hw4/pnli_test_unlabeled.csv', header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nlpaug\n",
        "import transformers\n",
        "import sentencepiece\n",
        "import nlpaug.augmenter.word as naw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x, y, label = [], [], []\n",
        "for i in range(len(train_df)):\n",
        "  data = train_df.iloc[i]\n",
        "  x.append(data[0]) \n",
        "  y.append(data[1])\n",
        "  label.append(int(data[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "back_translation_aug = naw.BackTranslationAug(from_model_name = \"Helsinki-NLP/opus-mt-en-ar\", to_model_name = \"Helsinki-NLP/opus-mt-ar-en\", device = device )\n",
        "x_aug = back_translation_aug.augment(x)\n",
        "y_aug = back_translation_aug.augment(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_aug_df = pd.DataFrame(x_aug)\n",
        "y_aug_df = pd.DataFrame(y_aug)\n",
        "label_df = pd.DataFrame(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat([x_aug_df, y_aug_df], axis=1)\n",
        "aug_df = pd.concat([df, label_df], axis=1)\n",
        "#after saving the data, manually update the excel file to match with the training file\n",
        "aug_df.to_csv(\"/content/drive/My Drive/hw4/back_translated_augmented_data_ar.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vYZA1zq4_e7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('/content/drive/My Drive/hw4/pnli_val_train.csv', header = None)\n",
        "# val_df   = pd.read_csv('/content/drive/My Drive/hw4/pnli_dev.csv', header = None)\n",
        "test_df  = pd.read_csv('/content/drive/My Drive/hw4/pnli_test_unlabeled.csv', header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hgO0sghg2rC"
      },
      "outputs": [],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EAX8W-qsVpN"
      },
      "outputs": [],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDrTr0aSqSBX"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
        "\n",
        "class DataDeBerta(Dataset):\n",
        "    def __init__(self, train_df, test_df):\n",
        "\n",
        "        self.train_df = train_df\n",
        "        self.test_df = test_df\n",
        "        self.train_data = None\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large', do_lower_case = True)\n",
        "        self.init_data()\n",
        "\n",
        "    def init_data(self):\n",
        "        self.train_data = self.load_data(self.train_df)\n",
        "        self.test_data = self.load_test_data(self.test_df)\n",
        "\n",
        "     def init_data(self):\n",
        "        self.train_data = self.load_data(self.train_df)\n",
        "        self.test_data = self.load_test_data(self.test_df)\n",
        "\n",
        "    def load_data(self, df):\n",
        "        MAX_LEN = 512\n",
        "        token_ids, mask_ids, seg_ids, y = [], [], [], []\n",
        "\n",
        "        precondition_list = df[0].to_list()\n",
        "        sentence_list = df[1].to_list()\n",
        "        label_list = df[2].to_list()\n",
        "\n",
        "        for (precondition, sentence, label) in zip(precondition_list, sentence_list, label_list):\n",
        "            precondition_id = self.tokenizer.encode(precondition, add_special_tokens = False)\n",
        "            sentence_id = self.tokenizer.encode(sentence, add_special_tokens = False)\n",
        "            pair_token_ids = [self.tokenizer.cls_token_id] + precondition_id + [self.tokenizer.sep_token_id] + sentence_id + [self.tokenizer.sep_token_id]\n",
        "            precondition_len = len(precondition_id)\n",
        "            sentence_len = len(sentence_id)\n",
        "\n",
        "            segment_ids = torch.tensor([0] * (precondition_len + 2) + [1] * (sentence_len + 1)) \n",
        "            attention_mask_ids = torch.tensor([1] * (precondition_len + sentence_len + 3))  # mask padded values\n",
        "\n",
        "            token_ids.append(torch.tensor(pair_token_ids))\n",
        "            seg_ids.append(segment_ids)\n",
        "            mask_ids.append(attention_mask_ids)\n",
        "            y.append(int(label))\n",
        "\n",
        "        token_ids = pad_sequence(token_ids, batch_first=True)\n",
        "        mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
        "        seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
        "        y = torch.tensor(y)\n",
        "        dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
        "        print(len(dataset))\n",
        "        return dataset\n",
        "\n",
        "    def load_test_data(self, df):\n",
        "        MAX_LEN = 512\n",
        "        token_ids, mask_ids, segment_ids, y = [], [], [], []\n",
        "\n",
        "        premise_list = df[0].to_list()\n",
        "        entailment_list = df[1].to_list()\n",
        "\n",
        "        for (premise, entailment, label) in zip(premise_list, entailment_list, label_list):\n",
        "            premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
        "            entailment_id = self.tokenizer.encode(entailment, add_special_tokens = False)\n",
        "            pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + entailment_id + [self.tokenizer.sep_token_id]\n",
        "            premise_len, entailment_len = len(premise_id), len(entailment_id)\n",
        "\n",
        "            segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (entailment_len + 1)) \n",
        "            attention_mask_ids = torch.tensor([1] * (premise_len + entailment_len + 3))\n",
        "\n",
        "            token_ids.append(torch.tensor(pair_token_ids))\n",
        "            segment_ids.append(segment_ids)\n",
        "            mask_ids.append(attention_mask_ids)\n",
        "\n",
        "        token_ids = pad_sequence(token_ids, batch_first = True)\n",
        "        mask_ids = pad_sequence(mask_ids, batch_first = True)\n",
        "        segment_ids = pad_sequence(segment_ids, batch_first = True)\n",
        "        dataset = TensorDataset(token_ids, mask_ids, segment_ids)\n",
        "        # print(len(dataset))\n",
        "        return dataset\n",
        "\n",
        "    def get_data_loaders(self, batch_size = 32):\n",
        "        train_loader = DataLoader(self.train_data, batch_size = batch_size, shuffle = True)\n",
        "        test_loader = DataLoader(self.test_data, batch_size = batch_size)\n",
        "        return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJLxxTZFqfNw"
      },
      "outputs": [],
      "source": [
        "train_dataset = DataDeBerta(train_df, test_df)\n",
        "train_loader, test_loader = train_dataset.get_data_loaders(batch_size = 16)\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArvBOQm5qqBC"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large', num_labels = 2, ignore_mismatched_sizes = True)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCOtNc98qyqz"
      },
      "outputs": [],
      "source": [
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.005}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVyXfm8sq0NQ"
      },
      "outputs": [],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = 2e-5, correct_bias = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRv6T1XUq6cc"
      },
      "outputs": [],
      "source": [
        "def multi_acc(y_pred, y_test):\n",
        "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2iD2txorHnH"
      },
      "outputs": [],
      "source": [
        "class PreconditionInference:\n",
        "  def __init__(self, model, train_loadder, test_loader, optimizer, epochs = 3):\n",
        "    self.epochs = epochs\n",
        "    self.model = model\n",
        "    self.optimizer = optimizer\n",
        "    self.test_loader = test_loader\n",
        "    self.train_loader = train_loadder\n",
        "\n",
        "  def multi_acc(self, y_pred, y_test):\n",
        "    acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
        "    return acc\n",
        "\n",
        "  def train(self):\n",
        "    for epoch in range(self.epochs):\n",
        "        self.model.train()\n",
        "        total_train_loss, total_train_acc  = 0, 0\n",
        "        for batch_idx, (pair_token_ids, mask_ids, segment_ids, y) in enumerate(self.train_loader):\n",
        "            self.optimizer.zero_grad()\n",
        "            pair_token_ids = pair_token_ids.to(device)\n",
        "            mask_ids = mask_ids.to(device)\n",
        "            segment_ids = segment_ids.to(device)\n",
        "            labels = y.to(device)\n",
        "            loss, prediction = self.model(pair_token_ids, token_type_ids = segment_ids, attention_mask = mask_ids, labels = labels).values()\n",
        "            acc = self.multi_acc(prediction, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            total_train_loss += loss.item()\n",
        "            total_train_acc  += acc.item()\n",
        "\n",
        "        train_acc  = total_train_acc/len(self.train_loader)\n",
        "        train_loss = total_train_loss/len(self.train_loader)\n",
        "        self.model.eval()\n",
        "        print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f}')\n",
        "\n",
        "  def predict(self):\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (pair_token_ids, mask_ids, seg_ids) in enumerate(self.test_loader):\n",
        "            pair_token_ids = pair_token_ids.to(device)\n",
        "            mask_ids = mask_ids.to(device)\n",
        "            seg_ids = seg_ids.to(device)\n",
        "            prediction = self.model(pair_token_ids, token_type_ids = seg_ids, attention_mask = mask_ids).values()\n",
        "            for pred in prediction:\n",
        "              predictions.append(pred)\n",
        "\n",
        "    data = []\n",
        "    for prediction in predictions:\n",
        "      for pred in prediction:\n",
        "        data.append(pred)\n",
        "    labels = [ 0 if label[0]>label[1] else 1 for label in data ]\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQlDRa6_t2zv"
      },
      "outputs": [],
      "source": [
        "preconditionInference = PreconditionInference(model, train_loader, test_loader, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxSEMKzZuQCo"
      },
      "outputs": [],
      "source": [
        "preconditionInference.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xM0gNsejSVl"
      },
      "outputs": [],
      "source": [
        "predictions = preconditionInference.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ea_EZQPufXRy"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/My Drive/hw4/upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
        "    for x in predictions:\n",
        "        fp.write(str(x) + '\\n')"
      ]
    }
  ]
}