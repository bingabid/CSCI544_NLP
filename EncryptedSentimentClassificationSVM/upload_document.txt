Complete this document and upload along with your prediction results and your code.

### Method Name ###
Use a two-part name to describe your method, e.g. bag-of-words + FFNN, cipherword word2vec + BiLSTM, etc.
    TDIDF and Suppot Vector Machine(SVM)

### Representation of sentence ###
Use up to 3 sentences to describe how you obtained the representation/features of each ciphertext sentence. E.g., bag-of-words? trained a word2vec or fasttext on all sentences from scratch?
    Used term frequency–inverse document frequency (tf-idf) for entire data set using existing package (TfidfVectorizer) 
    to represent the training sentences. 
### Classifier ###
Use up to 5 sentences to describe how you implemented your classifier. What encoder did you use and what was the learning objective?
    Used term frequency–inverse document frequency (tf-idf) for entire data set using existing package : TfidfVectorizer
    The key values to look for was the minimum-frequency and maximum-frequency to skip stop word/puntuation mark/high frequncy word.
    Used in built SVM classifier from sklearn: SVC

### Training & Development ###
Up to 5 sentences: how did you evaluate your solution using the dev set before submitting to the leaderboard? What are some key hyperparameter values (e.g., optimizer, learning rate, batch size, etc.)? How did you terminate the training (using a fixed #epochs, early stopping based on dev set performance)?
    I applied the gridsearch for Support Vector Machine to fine tune the hyperparameters (C, kernals). 
    Final result of the grid search shows, the results are opitmal for C = [6,8] with rbf kernal and was reporting 89.63% accuracy. 
    To train the overall model, I combined all the training and dev data into one. After combining, I applied k-fold cross-validation.
    Gradually incremented the K from 5 to 40, and saw the accuracy was about 89 to 90%. This accuracy at high value of K, proves direction is right
    Finally, trained the model using entire data set with C = 8 and kernal = rbf
    
### Other methods ###
Did you try other methods than the submitted one?
    I have tried following methods:
        1. Adaboost with linear perceptron as base algorithms
        2. Adaboost with SVM as the base algorithms
        3. Adaboost with Decision Tree as base algorithms
        4. Sochastic Gradient Decent
        5. Support Vector Machines with different kernal and C value using grid search
        6. Lstm Bideirection and normal Lstm

### Packages ###
List the key python packages you have used in this assignment.
    I have used the following packages on the assignment I am submitting
        1. from sklearn.svm import SVC
        2. from sklearn.model_selection import cross_val_score
        3. from sklearn.feature_extraction.text import TfidfVectorizer
    On other part which I did not submit here, I used following packages:
        1. import numpy as np
        2. adaboost packages
        3. from gensim.models import Word2Vec
        4. from keras.models import Sequential
        5. from collections import defaultdict
        6. from tensorflow.keras import optimizers
        7. from keras.initializers import Constant
        8. from keras.preprocessing import sequence
        9. from keras.layers.embeddings import Embedding
        10. from keras.layers import LSTM, Dropout, Dense, Flatten, Bidirectional`