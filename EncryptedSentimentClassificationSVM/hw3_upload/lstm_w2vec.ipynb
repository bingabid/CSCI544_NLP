{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "16220\n"
    }
   ],
   "source": [
    "for x in open('./train_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    train.append(x)\n",
    "print (len(train))\n",
    "# print (train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2027\n"
    }
   ],
   "source": [
    "for x in open('./dev_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    dev.append(x)\n",
    "print (len(dev))\n",
    "# print (dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2028\n"
    }
   ],
   "source": [
    "for x in open('./test_enc_unlabeled.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r')\n",
    "    test.append(x)\n",
    "print (len(test))\n",
    "# print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can split every sentence into lists of words by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = [[x[0], x[1].split(' ')] for x in train]\n",
    "dev_split = [[x[0], x[1].split(' ')] for x in dev]\n",
    "test_split = [[x.split(' ')] for x in test]\n",
    "# train_split[:2]\n",
    "# dev_split[:2]\n",
    "# test_split[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually, results need to be a list of 2028 0 or 1's\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.random.seed(7)\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM, Dropout, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = [item[1] for item in train_split]\n",
    "# train_sentence[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2Vec_model = Word2Vec(sentences = train_sentences, vector_size = 100, window = 10, min_count = 1, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20860\n20860 100\n20860\n"
    }
   ],
   "source": [
    "# w2v_weights = word2vec_model.wv.vectors\n",
    "# w2v_vocab_size, w2v_embedding_size = w2v_weights.shape\n",
    "# print(w2v_vocab_size, w2v_embedding_size)\n",
    "# vocab = list(word2vec_model.wv.index_to_key)\n",
    "# len(vocab)\n",
    "word2Vec_weights = word2Vec_model.wv.vectors\n",
    "word2Vec_vocab_size, word2Vec_embedding_size = word2Vec_weights.shape\n",
    "vocabulary = list(word2Vec_model.wv.index_to_key)\n",
    "\n",
    "word2Vec_dict = {}\n",
    "for word in vocabulary:\n",
    "    word2Vec_dict[word] = word2Vec_model.wv.get_vector(word)\n",
    "    \n",
    "print(len(word2Vec_dict))\n",
    "len(word2Vec_dict['.'])\n",
    "print(word2Vec_vocab_size, word2Vec_embedding_size)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20860\n"
    },
    {
     "data": {
      "text/plain": "100"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_vec_dict = {}\n",
    "# for word in vocab:\n",
    "#     word_vec_dict[word] = word2vec_model.wv.get_vector(word) \n",
    "# print(len(word_vec_dict))\n",
    "# len(word_vec_dict['.'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "56 ['lkê', 'xt8ö', 'tc', 'xjtlkxo8', 'lw', 'ükjl', 'wóê', 'é#êcoöêc', 'j#ê', 'lkê', 'Úwwm', 'zc', 'lütó', 'é#êötcêc', '77', 'lkjl', 'üê', 'Úêáwöê', 'ükw', 'üê', 'j#ê', 'wó', 'lkê', 'Újámc', 'wx', 'wo#', 'éj#êólc', ',', 'Úol', 'üê', 'kj2ê', 'ów', 't6êj', 'ükw', 'lkêú', 'üê#ê', 'jl', 'wo#', 'jyê', '.', 'jó6', 'lkjl', 'ltöê', 'tc', 'j', 'x8êêltóy', 'jó6', 'é#êátwoc', 'áwööw6tlú', 'ów', 'öjllê#', 'kwü', 'w86', 'úwo', 'j#ê', '.']\n"
    }
   ],
   "source": [
    "# getting maximum length sentence\n",
    "maxList = max(train_sentences, key = lambda i: len(i))\n",
    "max_sentence_length = len(maxList)\n",
    "print(max_sentence_length, maxList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "16220\n"
    },
    {
     "data": {
      "text/plain": "16220"
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [item[1] for item in train_split]\n",
    "Y_train = [item[0] for item in train_split]\n",
    "# print(X_train[:2])\n",
    "# print(Y_train[:4])\n",
    "print(len(X_train))\n",
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20860\n"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "train_dict = defaultdict(int)\n",
    "for i, sentence in enumerate(X_train):\n",
    "    for word in sentence:\n",
    "        train_dict[word] += 1\n",
    "\n",
    "print(len(train_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20856\n"
    }
   ],
   "source": [
    "min_freq, max_freq = 1, 10000\n",
    "train_dict = {k:v for k, v in train_dict.items() if v>=min_freq and v<=max_freq}\n",
    "print(len(train_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20856\n"
    }
   ],
   "source": [
    "rank_words = {key: rank for rank, key in enumerate(sorted(train_dict, key=train_dict.get, reverse=True), 1)}\n",
    "print(len(rank_words))\n",
    "vocabulary_size = len(rank_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "18"
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded = []\n",
    "for i,sentence in enumerate(X_train):\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        encoded_sentence.append(rank_words.get(word,0))\n",
    "    X_train_encoded.append(encoded_sentence)\n",
    "\n",
    "len(X_train_encoded[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2027"
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev = [item[1] for item in dev_split]\n",
    "y_test = [item[0] for item in dev_split]\n",
    "len(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2027"
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dev_encoded = []\n",
    "for i, sentence in enumerate(X_dev):\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        encoded_sentence.append(rank_words.get(word,0))\n",
    "    X_dev_encoded.append(encoded_sentence)\n",
    "\n",
    "len(X_dev_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,  194, 4529,   10,  163,    0,    5,   10,    0,  212,\n       2069, 9249,  184,    0,  828,    2,    0,  862,    0,  196,    0,\n       2529, 6078,   25,   42,  397,    1,    0,  804,  886,   42, 1651,\n          0], dtype=int32)"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# truncate and pad input sequences\n",
    "X_train = sequence.pad_sequences(X_train_encoded, maxlen = max_sentence_length)\n",
    "X_test = sequence.pad_sequences(X_dev_encoded, maxlen = max_sentence_length)\n",
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "Y_train = np.asarray(Y_train)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(20857, 100)\n"
    }
   ],
   "source": [
    "# now creating the embedding matrix\n",
    "embedding_matrix = np.zeros(shape=(vocabulary_size + 1, word2Vec_embedding_size))\n",
    "for i,word in enumerate(rank_words):\n",
    "  embedding_vector = word2Vec_dict.get(word)\n",
    "  if embedding_vector is not None:  # word is in the vocabulary learned by the w2v model\n",
    "    embedding_matrix[i+1] = embedding_vector\n",
    "  # if word is not found then embed_vector corressponding to that vector will stay zero.\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[-0.3859272   0.29502597  0.5025096  -0.00436226 -0.27154326 -0.10293099\n  0.32060978  0.2423444  -0.1385685  -0.50439054 -0.0077074  -0.1548165\n -0.02441715  0.08030856  0.1301237   0.07277843  0.42257386  0.11222274\n -0.14543763 -0.68495     0.36744437 -0.13969915  0.28065568 -0.2201954\n -0.16848989 -0.01974002 -0.26097727 -0.04706571 -0.04970746  0.38222367\n  0.22387798 -0.08790313  0.10107074 -0.48340166  0.17058302  0.26408052\n  0.28522012  0.0994729  -0.514254   -0.05467613  0.24629626 -0.38426045\n -0.21269956  0.356397   -0.09157103 -0.12774144  0.14562462 -0.30157048\n  0.40542898  0.22734295  0.06030554 -0.22287962 -0.26009795 -0.15791777\n -0.46795398 -0.01119609  0.15155265 -0.00257432 -0.252003    0.41979232\n -0.01622939  0.00529151  0.41018394 -0.12429683 -0.14171688  0.56592745\n  0.12852614  0.15051155 -0.42725107 -0.00219187 -0.17311488  0.07736689\n  0.18298632  0.29209015  0.32596895  0.06871334  0.04326439  0.24928945\n -0.3235982   0.02513141 -0.23781468 -0.02458365 -0.2392898   0.08604681\n  0.0056574  -0.05465946  0.4535882  -0.14212494  0.37832627 -0.01792237\n  0.26524025  0.03149629  0.11576737  0.16576084  0.48621058 -0.19658467\n  0.22290708 -0.16662383  0.05971506  0.09378178]\n"
    },
    {
     "data": {
      "text/plain": "array([-0.3859272 ,  0.29502597,  0.50250959, -0.00436226, -0.27154326,\n       -0.10293099,  0.32060978,  0.24234439, -0.13856851, -0.50439054,\n       -0.0077074 , -0.15481649, -0.02441715,  0.08030856,  0.1301237 ,\n        0.07277843,  0.42257386,  0.11222274, -0.14543763, -0.68494999,\n        0.36744437, -0.13969915,  0.28065568, -0.2201954 , -0.16848989,\n       -0.01974002, -0.26097727, -0.04706571, -0.04970746,  0.38222367,\n        0.22387798, -0.08790313,  0.10107074, -0.48340166,  0.17058302,\n        0.26408052,  0.28522012,  0.0994729 , -0.51425397, -0.05467613,\n        0.24629626, -0.38426045, -0.21269956,  0.356397  , -0.09157103,\n       -0.12774144,  0.14562462, -0.30157048,  0.40542898,  0.22734295,\n        0.06030554, -0.22287962, -0.26009795, -0.15791777, -0.46795398,\n       -0.01119609,  0.15155265, -0.00257432, -0.25200301,  0.41979232,\n       -0.01622939,  0.00529151,  0.41018394, -0.12429683, -0.14171688,\n        0.56592745,  0.12852614,  0.15051155, -0.42725107, -0.00219187,\n       -0.17311488,  0.07736689,  0.18298632,  0.29209015,  0.32596895,\n        0.06871334,  0.04326439,  0.24928945, -0.32359821,  0.02513141,\n       -0.23781468, -0.02458365, -0.23928981,  0.08604681,  0.0056574 ,\n       -0.05465946,  0.45358819, -0.14212494,  0.37832627, -0.01792237,\n        0.26524025,  0.03149629,  0.11576737,  0.16576084,  0.48621058,\n       -0.19658467,  0.22290708, -0.16662383,  0.05971506,  0.09378178])"
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word2Vec_model.wv.get_vector('y#êjl'))\n",
    "embedding_matrix[rank_words['y#êjl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/5\n507/507 [==============================] - 25s 46ms/step - loss: 0.5446 - accuracy: 0.7216 - val_loss: 0.3698 - val_accuracy: 0.8451\nEpoch 2/5\n507/507 [==============================] - 23s 46ms/step - loss: 0.2966 - accuracy: 0.8816 - val_loss: 0.3156 - val_accuracy: 0.8678\nEpoch 3/5\n507/507 [==============================] - 25s 50ms/step - loss: 0.1725 - accuracy: 0.9354 - val_loss: 0.3211 - val_accuracy: 0.8796\nEpoch 4/5\n507/507 [==============================] - 25s 49ms/step - loss: 0.1191 - accuracy: 0.9586 - val_loss: 0.3356 - val_accuracy: 0.8860\nEpoch 5/5\n507/507 [==============================] - 24s 48ms/step - loss: 0.0876 - accuracy: 0.9684 - val_loss: 0.3548 - val_accuracy: 0.8905\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x15e42ffa0>"
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocabulary_size + 1, output_dim = word2Vec_embedding_size, input_length = max_sentence_length, embeddings_initializer = Constant(embedding_matrix)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "model.fit(X_train, Y_train, validation_data = (X_test, y_test), epochs = 5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[835 117]\n [106 969]]\n              precision    recall  f1-score   support\n\n           0       0.89      0.88      0.88       952\n           1       0.89      0.90      0.90      1075\n\n    accuracy                           0.89      2027\n   macro avg       0.89      0.89      0.89      2027\nweighted avg       0.89      0.89      0.89      2027\n\n0.889985199802664\n"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "y_dev_prob = model.predict(X_test)\n",
    "y_dev_lstm = []\n",
    "for prob in y_dev_prob:\n",
    "    if prob >= 0.49:\n",
    "        y_dev_lstm.append(1)\n",
    "    else:\n",
    "        y_dev_lstm.append(0)\n",
    "# print(y_dev_lstm)\n",
    "print(confusion_matrix(y_test,y_dev_lstm))\n",
    "print(classification_report(y_test,y_dev_lstm))\n",
    "print(accuracy_score(y_test, y_dev_lstm))\n",
    "with open('lstm.txt', 'w') as wf:\n",
    "    for item in y_dev_lstm:\n",
    "        wf.write(str(item) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/5\n254/254 [==============================] - 26s 94ms/step - loss: 0.5327 - accuracy: 0.7187 - val_loss: 0.3690 - val_accuracy: 0.8416\nEpoch 2/5\n254/254 [==============================] - 24s 95ms/step - loss: 0.2498 - accuracy: 0.9038 - val_loss: 0.3318 - val_accuracy: 0.8703\nEpoch 3/5\n254/254 [==============================] - 23s 89ms/step - loss: 0.1471 - accuracy: 0.9494 - val_loss: 0.3306 - val_accuracy: 0.8860\nEpoch 4/5\n254/254 [==============================] - 21s 81ms/step - loss: 0.1036 - accuracy: 0.9649 - val_loss: 0.3331 - val_accuracy: 0.8816\nEpoch 5/5\n254/254 [==============================] - 20s 81ms/step - loss: 0.0798 - accuracy: 0.9711 - val_loss: 0.3837 - val_accuracy: 0.8846\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x131edc730>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=w2v_vocab_size+1,output_dim=w2v_embedding_size,input_length=max_sentence_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python310064bitfb335e516cf1419fa40eefd43c9b328c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}