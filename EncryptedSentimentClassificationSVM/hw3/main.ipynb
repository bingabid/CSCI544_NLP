{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open('./train_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    train.append(x)\n",
    "# print (len(train))\n",
    "# print (train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open('./dev_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    dev.append(x)\n",
    "# print (len(dev))\n",
    "# print (dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in open('./test_enc_unlabeled.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r')\n",
    "    test.append(x)\n",
    "# print (len(test))\n",
    "# print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### You can split every sentence into lists of words by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0, 'lkêcê yoúc cêêö y#êjl lw mówám Újám j Úêê# ütlk Úol lkêú z#ê ctöé8ú ówl xoóóú éê#xw#öê#c .'], [0, '6êcétlê jolêot8 zc éê#xw#öjóáê , tl zc j #jlkê# 8tcl8êcc jöÚ8ê 6wüó lkê öt668ê wx lkê #wj6 , ükê#ê lkê lkêöjltá t#wótêc j#ê lww wÚ2twoc jó6 lkê cê+oj8 éw8tltác lww cöoy .']]\n[[1, 'ów8jó Ú#j2ê8ú l#êj6c ükê#ê xêü jöê#tájó xt8öc 6j#ê lw 6ê82ê 77 tólw lkê üw#86 wx jöÚt2j8êóáê jó6 jöÚtyotlú <<<'], [0, 'ê2êó öo#ékú zc ê+éê#l áwötá ltötóy jó6 xjöê6 ákj#tcöj áj ózl #êcáoê lktc êxxw#l .']]\n['j 6t6jáltá jó6 6o88 6wáoöêólj#ú y8w#txútóy cwxlüj#ê jój#ákú .', 'ówlktóy cltámc , #êj88ú , ê+áêél j 8tóyê#tóy á#êêétóêcc wóê xêê8c x#wö Úêtóy 6#jyyê6 lk#woyk j cj6 , cw#6t6 oót2ê#cê wx yoóc , 6#oyc , j2j#táê jó6 6jöjyê6 6#êjöc .']\n"
    }
   ],
   "source": [
    "print(train[:2])\n",
    "print(dev[:2])\n",
    "print(test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = [[x[0], x[1].split(' ')] for x in train]\n",
    "dev_split = [[x[0], x[1].split(' ')] for x in dev]\n",
    "test_split = [x.split(' ') for x in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0, ['lkêcê', 'yoúc', 'cêêö', 'y#êjl', 'lw', 'mówám', 'Újám', 'j', 'Úêê#', 'ütlk', 'Úol', 'lkêú', 'z#ê', 'ctöé8ú', 'ówl', 'xoóóú', 'éê#xw#öê#c', '.']], [0, ['6êcétlê', 'jolêot8', 'zc', 'éê#xw#öjóáê', ',', 'tl', 'zc', 'j', '#jlkê#', '8tcl8êcc', 'jöÚ8ê', '6wüó', 'lkê', 'öt668ê', 'wx', 'lkê', '#wj6', ',', 'ükê#ê', 'lkê', 'lkêöjltá', 't#wótêc', 'j#ê', 'lww', 'wÚ2twoc', 'jó6', 'lkê', 'cê+oj8', 'éw8tltác', 'lww', 'cöoy', '.']]]\n[[1, ['ów8jó', 'Ú#j2ê8ú', 'l#êj6c', 'ükê#ê', 'xêü', 'jöê#tájó', 'xt8öc', '6j#ê', 'lw', '6ê82ê', '77', 'tólw', 'lkê', 'üw#86', 'wx', 'jöÚt2j8êóáê', 'jó6', 'jöÚtyotlú', '<<<']], [0, ['ê2êó', 'öo#ékú', 'zc', 'ê+éê#l', 'áwötá', 'ltötóy', 'jó6', 'xjöê6', 'ákj#tcöj', 'áj', 'ózl', '#êcáoê', 'lktc', 'êxxw#l', '.']]]\n[['j', '6t6jáltá', 'jó6', '6o88', '6wáoöêólj#ú', 'y8w#txútóy', 'cwxlüj#ê', 'jój#ákú', '.'], ['ówlktóy', 'cltámc', ',', '#êj88ú', ',', 'ê+áêél', 'j', '8tóyê#tóy', 'á#êêétóêcc', 'wóê', 'xêê8c', 'x#wö', 'Úêtóy', '6#jyyê6', 'lk#woyk', 'j', 'cj6', ',', 'cw#6t6', 'oót2ê#cê', 'wx', 'yoóc', ',', '6#oyc', ',', 'j2j#táê', 'jó6', '6jöjyê6', '6#êjöc', '.']]\n"
    }
   ],
   "source": [
    "print(train_split[0:2])\n",
    "print(dev_split[0:2])\n",
    "print(test_split[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required library\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM, Dropout, Dense, Flatten, Bidirectional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using Word2Vec to create vector embedding\n",
    "train_sentences = [item[1] for item in train_split]\n",
    "maxList = max(train_sentences, key = lambda i: len(i))\n",
    "max_sentence_length = len(maxList)\n",
    "\n",
    "word2Vec_model = Word2Vec(sentences = train_sentences, vector_size = 100, window = 10, min_count = 1, sg = 1)\n",
    "word2Vec_weights = word2Vec_model.wv.vectors\n",
    "word2Vec_vocab_size, word2Vec_embedding_size = word2Vec_weights.shape\n",
    "vocabulary = list(word2Vec_model.wv.index_to_key)\n",
    "\n",
    "word2Vec_dict = {}\n",
    "for word in vocabulary:\n",
    "    word2Vec_dict[word] = word2Vec_model.wv.get_vector(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2027\n[['ów8jó', 'Ú#j2ê8ú', 'l#êj6c', 'ükê#ê', 'xêü', 'jöê#tájó', 'xt8öc', '6j#ê', 'lw', '6ê82ê', '77', 'tólw', 'lkê', 'üw#86', 'wx', 'jöÚt2j8êóáê', 'jó6', 'jöÚtyotlú', '<<<'], ['ê2êó', 'öo#ékú', 'zc', 'ê+éê#l', 'áwötá', 'ltötóy', 'jó6', 'xjöê6', 'ákj#tcöj', 'áj', 'ózl', '#êcáoê', 'lktc', 'êxxw#l', '.']]\n2028\n[['j', '6t6jáltá', 'jó6', '6o88', '6wáoöêólj#ú', 'y8w#txútóy', 'cwxlüj#ê', 'jój#ákú', '.'], ['ówlktóy', 'cltámc', ',', '#êj88ú', ',', 'ê+áêél', 'j', '8tóyê#tóy', 'á#êêétóêcc', 'wóê', 'xêê8c', 'x#wö', 'Úêtóy', '6#jyyê6', 'lk#woyk', 'j', 'cj6', ',', 'cw#6t6', 'oót2ê#cê', 'wx', 'yoóc', ',', '6#oyc', ',', 'j2j#táê', 'jó6', '6jöjyê6', '6#êjöc', '.']]\n"
    }
   ],
   "source": [
    "X_dev = [item[1] for item in dev_split]\n",
    "Y_dev = [item[0] for item in dev_split]\n",
    "X_test = test_split\n",
    "X_train = [item[1] for item in train_split]\n",
    "Y_train = [item[0] for item in train_split]\n",
    "\n",
    "print(len(X_dev))\n",
    "print(X_dev[0:2])\n",
    "\n",
    "print(len(X_test))\n",
    "print(X_test[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prepocessing\n",
    "X_dev = [item[1] for item in dev_split]\n",
    "Y_dev = [item[0] for item in dev_split]\n",
    "# X_test = [item[0] for item in test_split]\n",
    "X_test =  test_split\n",
    "X_train = [item[1] for item in train_split]\n",
    "Y_train = [item[0] for item in train_split]\n",
    "\n",
    "train_dict = defaultdict(int)\n",
    "for i, sentence in enumerate(X_train):\n",
    "    for word in sentence:\n",
    "        train_dict[word] += 1\n",
    "\n",
    "# print(len(train_dict))\n",
    "# print(len(vocabulary))\n",
    "\n",
    "min_freq, max_freq = 1, 8000\n",
    "train_dict = {k:v for k, v in train_dict.items() if v>=min_freq and v<=max_freq}\n",
    "rank_words = {key: rank for rank, key in enumerate(sorted(train_dict, key=train_dict.get, reverse=True), 1)}\n",
    "vocabulary_size = len(rank_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "20854\n20860\n\n"
    }
   ],
   "source": [
    "print(len(train_dict))\n",
    "print(len(vocabulary))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mbedding matrix\n",
    "embedding_matrix = np.zeros(shape=(vocabulary_size + 1, word2Vec_embedding_size))\n",
    "for i,word in enumerate(rank_words):\n",
    "  embedding_vector = word2Vec_dict.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[i+1] = embedding_vector\n",
    "# print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(20855, 100)\n"
    }
   ],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data encoding for training\n",
    "X_train_encoded = []\n",
    "for i,sentence in enumerate(X_train):\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        encoded_sentence.append(rank_words.get(word,0))\n",
    "    X_train_encoded.append(encoded_sentence)\n",
    "\n",
    "X_dev_encoded = []\n",
    "for i, sentence in enumerate(X_dev):\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        encoded_sentence.append(rank_words.get(word,0))\n",
    "    X_dev_encoded.append(encoded_sentence)\n",
    "\n",
    "X_test_encoded = []\n",
    "for i, sentence in enumerate(X_test):\n",
    "    encoded_sentence = []\n",
    "    for word in sentence:\n",
    "        encoded_sentence.append(rank_words.get(word,0))\n",
    "    X_test_encoded.append(encoded_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data padding\n",
    "X_train_pad = sequence.pad_sequences(X_train_encoded, maxlen = max_sentence_length)\n",
    "X_dev_pad = sequence.pad_sequences(X_dev_encoded, maxlen = max_sentence_length)\n",
    "X_test_pad = sequence.pad_sequences(X_test_encoded, maxlen = max_sentence_length)\n",
    "\n",
    "X_train_sample = np.asarray(X_train_pad)\n",
    "X_dev_sample = np.asarray(X_dev_pad)\n",
    "Y_train_sample = np.asarray(Y_train)\n",
    "Y_dev_sample = np.asarray(Y_dev)\n",
    "X_total_sample = np.concatenate((X_train_sample, X_dev_sample), axis=0)\n",
    "Y_total_sample = np.concatenate((Y_train_sample, Y_dev_sample), axis=0)\n",
    "X_test_sample = np.asarray(X_test_pad)\n",
    "\n",
    "# print(len(X_total_sample))\n",
    "# print(X_total_sample.shape)\n",
    "# print(len(Y_total_sample))\n",
    "# print(X_test_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1/4\n1014/1014 [==============================] - 87s 83ms/step - loss: 0.5139 - accuracy: 0.7427 - val_loss: 0.3587 - val_accuracy: 0.8505\nEpoch 2/4\n1014/1014 [==============================] - 63s 62ms/step - loss: 0.2634 - accuracy: 0.8950 - val_loss: 0.3059 - val_accuracy: 0.8747\nEpoch 3/4\n1014/1014 [==============================] - 56s 56ms/step - loss: 0.1553 - accuracy: 0.9438 - val_loss: 0.3235 - val_accuracy: 0.8865\nEpoch 4/4\n1014/1014 [==============================] - 60s 59ms/step - loss: 0.1037 - accuracy: 0.9639 - val_loss: 0.3363 - val_accuracy: 0.8905\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x12d3cdd50>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocabulary_size + 1, output_dim = word2Vec_embedding_size, input_length = max_sentence_length, embeddings_initializer = Constant(embedding_matrix)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "# model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "model.fit(X_train_sample, Y_train_sample, validation_data = (X_dev_sample, Y_dev_sample), epochs = 4, batch_size = 16)\n",
    "# model.fit(X_total_sample, Y_total_sample, epochs = 5, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually, results need to be a list of 2028 0 or 1's\n",
    "results = []\n",
    "result_probabilities = model.predict(X_test_sample)\n",
    "results = [1 if probability>=0.5 else 0 for probability in result_probabilities]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1023\n1005\n"
    }
   ],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]\n",
    "zero = results.count(0)\n",
    "one = results.count(1)\n",
    "print(zero)\n",
    "print(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions1.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python310064bitfb335e516cf1419fa40eefd43c9b328c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}